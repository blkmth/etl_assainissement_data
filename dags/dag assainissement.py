"""
DAG Airflow pour pipeline ETL d'assainissement de donnÃ©es.

ARCHITECTURE: SÃ©quentielle (pas de parallÃ©lisation)
FRÃ‰QUENCE: Quotidienne Ã  2h du matin
RETRY: 3 tentatives avec dÃ©lai de 5 minutes

FLUX:
start â†’ create_schemas â†’ extract_finances â†’ transform_finances â†’ load_finances
                      â†’ extract_ventes â†’ transform_ventes â†’ load_ventes â†’ end
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator

# Import des modules ETL
import sys
from pathlib import Path

# Ajouter le rÃ©pertoire racine du projet au PYTHONPATH
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.extract.mysql_extractor import MySQLExtractor
from src.transform.orchestrateur import transformer_table
from src.load.mysql_loader import MySQLLoader
from src.config.logging_config import setup_logger

logger = setup_logger(__name__)

# Configuration par dÃ©faut du DAG
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,  # Ne pas dÃ©pendre de l'exÃ©cution prÃ©cÃ©dente
    'start_date': datetime(2025, 12, 1),
    'email': ['data@votreentreprise.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,  # 3 tentatives en cas d'Ã©chec
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2)  # Timeout Ã  2h
}

# DÃ©finition du DAG
dag = DAG(
    'etl_assainissement_donnees',
    default_args=default_args,
    description='Pipeline ETL pour assainissement donnÃ©es financiÃ¨res et ventes',
    schedule_interval='0 2 * * *',  # Tous les jours Ã  2h du matin
    catchup=False,  # Ne pas exÃ©cuter les runs manquÃ©s
    max_active_runs=1,  # Une seule exÃ©cution Ã  la fois
    tags=['etl', 'assainissement', 'production']
)

# ============= FONCTIONS PYTHON POUR CHAQUE TÃ‚CHE =============

def create_database_schemas(**context):
    """TÃ¢che 1: CrÃ©er les schÃ©mas de base de donnÃ©es cibles"""
    logger.info("="*50)
    logger.info("TÃ‚CHE: CrÃ©ation des schÃ©mas de base de donnÃ©es")
    logger.info("="*50)
    
    loader = MySQLLoader()
    loader.create_tables_if_not_exist()
    
    logger.info("âœ“ SchÃ©mas crÃ©Ã©s avec succÃ¨s")

def extract_finances_task(**context):
    """TÃ¢che 2: Extraire les donnÃ©es financiÃ¨res"""
    logger.info("="*50)
    logger.info("TÃ‚CHE: Extraction donnÃ©es FINANCES")
    logger.info("="*50)
    
    extractor = MySQLExtractor()
    # Utiliser extract_table avec le nom de la table source
    # Note: Ajustez le nom de table selon votre schÃ©ma source
    df = extractor.extract_table('finances')  # ou le nom rÃ©el de votre table source
    
    # Sauvegarder dans XCom pour passer aux tÃ¢ches suivantes
    # Note: Pour gros volumes, utiliser stockage externe (S3, etc.)
    context['task_instance'].xcom_push(
        key='finances_raw', 
        value=df.to_json(orient='records')
    )
    
    logger.info(f"âœ“ {len(df)} lignes extraites et stockÃ©es dans XCom")

def transform_finances_task(**context):
    """TÃ¢che 3: Transformer les donnÃ©es financiÃ¨res"""
    logger.info("="*50)
    logger.info("TÃ‚CHE: Transformation donnÃ©es FINANCES")
    logger.info("="*50)
    
    import pandas as pd
    
    # RÃ©cupÃ©rer donnÃ©es depuis XCom
    finances_json = context['task_instance'].xcom_pull(
        task_ids='extract_finances',
        key='finances_raw'
    )
    df = pd.read_json(finances_json, orient='records')
    
    # Appliquer transformation avec le nouvel orchestrateur
    df_clean, metadata = transformer_table(
        df,
        'clients',  # ou 'finances' selon votre config
        chemin_config='src/transform/config.yaml'
    )
    
    # Extraire les statistiques depuis les mÃ©tadonnÃ©es
    stats = {
        'total_rows': metadata.get('nombre_lignes_final', len(df_clean)),
        'duplicates_removed': metadata.get('metadata_defaut', {}).get('lignes_supprimees_doublons', 0),
        'invalid_emails': 0,  # Ã€ adapter selon vos validations
        'invalid_years': 0,
        'quality_score': metadata.get('qualite', {}).get('taux_completude', 0)
    }
    
    # Stocker rÃ©sultat et mÃ©triques
    context['task_instance'].xcom_push(
        key='finances_clean',
        value=df_clean.to_json(orient='records')
    )
    context['task_instance'].xcom_push(
        key='finances_stats',
        value=stats
    )
    
    logger.info(f"âœ“ Transformation terminÃ©e: {len(df_clean)} lignes valides")
    logger.info(f"ğŸ“Š Statistiques: {stats}")

def load_finances_task(**context):
    """TÃ¢che 4: Charger les donnÃ©es financiÃ¨res"""
    logger.info("="*50)
    logger.info("TÃ‚CHE: Chargement donnÃ©es FINANCES")
    logger.info("="*50)
    
    import pandas as pd
    
    # RÃ©cupÃ©rer donnÃ©es nettoyÃ©es
    finances_json = context['task_instance'].xcom_pull(
        task_ids='transform_finances',
        key='finances_clean'
    )
    df = pd.read_json(finances_json, orient='records')
    
    # Charger dans base cible
    loader = MySQLLoader()
    rows_loaded = loader.load_finances(df)
    
    # Enregistrer mÃ©triques
    stats = context['task_instance'].xcom_pull(
        task_ids='transform_finances',
        key='finances_stats'
    )
    loader.load_quality_metrics(
        stats, 
        'finances', 
        context['execution_date'].strftime('%Y-%m-%d %H:%M:%S')
    )
    
    logger.info(f"âœ“ {rows_loaded} lignes chargÃ©es avec succÃ¨s")

def extract_ventes_task(**context):
    """TÃ¢che 5: Extraire les donnÃ©es de ventes"""
    logger.info("="*50)
    logger.info("TÃ‚CHE: Extraction donnÃ©es VENTES")
    logger.info("="*50)
    
    extractor = MySQLExtractor()
    # Utiliser extract_table avec le nom de la table source
    # Note: Ajustez le nom de table selon votre schÃ©ma source
    df = extractor.extract_table('ventes')  # ou le nom rÃ©el de votre table source
    
    context['task_instance'].xcom_push(
        key='ventes_raw',
        value=df.to_json(orient='records')
    )
    
    logger.info(f"âœ“ {len(df)} lignes extraites")

def transform_ventes_task(**context):
    """TÃ¢che 6: Transformer les donnÃ©es de ventes"""
    logger.info("="*50)
    logger.info("TÃ‚CHE: Transformation donnÃ©es VENTES")
    logger.info("="*50)
    
    import pandas as pd
    
    ventes_json = context['task_instance'].xcom_pull(
        task_ids='extract_ventes',
        key='ventes_raw'
    )
    df = pd.read_json(ventes_json, orient='records')
    
    # Appliquer transformation avec le nouvel orchestrateur
    df_clean, metadata = transformer_table(
        df,
        'vehicules',  # ou 'ventes' selon votre config
        chemin_config='src/transform/config.yaml'
    )
    
    # Extraire les statistiques depuis les mÃ©tadonnÃ©es
    stats = {
        'total_rows': metadata.get('nombre_lignes_final', len(df_clean)),
        'duplicates_removed': metadata.get('metadata_defaut', {}).get('lignes_supprimees_doublons', 0),
        'invalid_emails': 0,
        'invalid_years': 0,
        'quality_score': metadata.get('qualite', {}).get('taux_completude', 0)
    }
    
    context['task_instance'].xcom_push(
        key='ventes_clean',
        value=df_clean.to_json(orient='records')
    )
    context['task_instance'].xcom_push(
        key='ventes_stats',
        value=stats
    )
    
    logger.info(f"âœ“ Transformation terminÃ©e: {len(df_clean)} lignes valides")

def load_ventes_task(**context):
    """TÃ¢che 7: Charger les donnÃ©es de ventes"""
    logger.info("="*50)
    logger.info("TÃ‚CHE: Chargement donnÃ©es VENTES")
    logger.info("="*50)
    
    import pandas as pd
    
    ventes_json = context['task_instance'].xcom_pull(
        task_ids='transform_ventes',
        key='ventes_clean'
    )
    df = pd.read_json(ventes_json, orient='records')
    
    loader = MySQLLoader()
    rows_loaded = loader.load_ventes(df)
    
    stats = context['task_instance'].xcom_pull(
        task_ids='transform_ventes',
        key='ventes_stats'
    )
    loader.load_quality_metrics(
        stats,
        'ventes',
        context['execution_date'].strftime('%Y-%m-%d %H:%M:%S')
    )
    
    logger.info(f"âœ“ {rows_loaded} lignes chargÃ©es avec succÃ¨s")

# ============= DÃ‰FINITION DES TÃ‚CHES AIRFLOW =============

start = EmptyOperator(task_id='start', dag=dag)

create_schemas = PythonOperator(
    task_id='create_schemas',
    python_callable=create_database_schemas,
    dag=dag
)

extract_finances = PythonOperator(
    task_id='extract_finances',
    python_callable=extract_finances_task,
    dag=dag
)

transform_finances = PythonOperator(
    task_id='transform_finances',
    python_callable=transform_finances_task,
    dag=dag
)

load_finances = PythonOperator(
    task_id='load_finances',
    python_callable=load_finances_task,
    dag=dag
)

extract_ventes = PythonOperator(
    task_id='extract_ventes',
    python_callable=extract_ventes_task,
    dag=dag
)

transform_ventes = PythonOperator(
    task_id='transform_ventes',
    python_callable=transform_ventes_task,
    dag=dag
)

load_ventes = PythonOperator(
    task_id='load_ventes',
    python_callable=load_ventes_task,
    dag=dag
)

end = EmptyOperator(task_id='end', dag=dag)

# ============= DÃ‰FINITION DU FLUX SÃ‰QUENTIEL =============

start >> create_schemas

# Branche finances
create_schemas >> extract_finances >> transform_finances >> load_finances

# Branche ventes (aprÃ¨s finances)
load_finances >> extract_ventes >> transform_ventes >> load_ventes

load_ventes >> end
